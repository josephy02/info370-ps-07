{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Hello y'all see this?\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "GYWdslI-sTsX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhwSx51vSJgG",
        "outputId": "a63e65f8-f0da-4c8d-f91e-4cf45aa0480c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect Colab to Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip training data, run once\n",
        "#!unzip drive/MyDrive/data/train.zip -d drive/MyDrive/data/"
      ],
      "metadata": {
        "id": "mLLPHBB6WG6O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip validation data, run once\n",
        "#!unzip drive/MyDrive/data/validation.zip -d drive/MyDrive/data/"
      ],
      "metadata": {
        "id": "UW1xlsZzaW1b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code chunk puts all image paths and their labels in a dataframe\n",
        "train_paths = []\n",
        "train_labels = []\n",
        "validation_paths = []\n",
        "validation_labels = []\n",
        "troot = 'drive/MyDrive/data/train-cropped'\n",
        "vroot = 'drive/MyDrive/data/validation-cropped'\n",
        "\n",
        "for f in os.listdir(troot):\n",
        "  tpath = os.path.join(troot, f)\n",
        "  tlabel = f[-10:-8]\n",
        "  train_paths.append(tpath)\n",
        "  train_labels.append(tlabel)\n",
        "\n",
        "for f in os.listdir(vroot):\n",
        "  vpath = os.path.join(vroot, f)\n",
        "  vlabel = f[-10:-8]\n",
        "  validation_paths.append(vpath)\n",
        "  validation_labels.append(vlabel)\n",
        "\n",
        "train_images = pd.DataFrame({'path': train_paths,\n",
        "                             'label': train_labels})\n",
        "validation_images = pd.DataFrame({'path': validation_paths,\n",
        "                                  'label': validation_labels})"
      ],
      "metadata": {
        "id": "x-vpLAwseCZF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subset(n, data, seed=123):\n",
        "  random.seed(seed)\n",
        "  sub = random.sample(range(data.shape[0] + 1), n)\n",
        "  return data.iloc[sub,:]\n",
        "\n",
        "train_subset = get_subset(1000, train_images)\n",
        "validation_subset = get_subset(1000, validation_images)"
      ],
      "metadata": {
        "id": "H92_LMq-4xFX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imageWidth, imageHeight = 128, 128\n",
        "imageSize = (imageWidth, imageHeight)\n",
        "channel = 1\n",
        "\n"
      ],
      "metadata": {
        "id": "VELC0V_JzOzB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ").flow_from_dataframe(\n",
        "    dataframe = train_subset,\n",
        "    directory = None,\n",
        "    x_col='path',\n",
        "    y_col='label',\n",
        "    class_mode='categorical',  # target is 2-D array of one-hot encoded labels\n",
        "    target_size=imageSize,\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "validation_generator = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ").flow_from_dataframe(\n",
        "    dataframe = validation_subset,\n",
        "    directory = None,\n",
        "    x_col='path',\n",
        "    class_mode= None,  # target is 2-D array of one-hot encoded labels\n",
        "    target_size=imageSize,\n",
        "    shuffle=False\n",
        "    )"
      ],
      "metadata": {
        "id": "NXYyrEJgzQ9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e5009c-25b9-4e49-f4c2-c8e96ea7ed5c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1000 validated image filenames belonging to 5 classes.\n",
            "Found 1000 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nCategories = 5\n",
        "model = Sequential()\n",
        "## First convolutional layer with 32 filters (kernels)\n",
        "model.add(Conv2D(32,\n",
        "                 kernel_size=3,\n",
        "                 activation='relu',\n",
        "                 input_shape=(imageWidth, imageHeight, channel)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "## 2nd convolutional layer\n",
        "model.add(Conv2D(64,\n",
        "                 kernel_size = 3,\n",
        "                 activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "## 3rd convolutional layer\n",
        "model.add(Conv2D(128,\n",
        "                 kernel_size=3,\n",
        "                 activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "## Flatten the image into a string of pixels\n",
        "model.add(Flatten())\n",
        "## Use one final dense layer\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "## Output layer with 2 softmax nodes\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=1\n",
        ")\n",
        "train_phat = model.predict(train_generator)\n",
        "validation_phat = model.predict(validation_generator)\n",
        "label_map = {0:\"DA\", 1:\"EN\", 2:\"RU\", 3:\"TH\", 4:\"ZN\"}\n",
        "\n",
        "train_predictions = np.argmax(train_phat, axis=-1)\n",
        "validation_predictions = np.argmax(validation_phat, axis=-1)\n",
        "\n",
        "train_true_lables = train_subset['label'].replace(label_map)\n",
        "validation_true_lables = train_subset['label'].replace(label_map)\n",
        "\n",
        "tacc = np.sum([1 if train_predictions[i] == train_true_labels[i] else 0 for i in range(len(train_predictions))])\n",
        "tacc"
      ],
      "metadata": {
        "id": "WiWL0gtE2ED5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}